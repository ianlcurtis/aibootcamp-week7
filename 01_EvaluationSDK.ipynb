{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Required Libraries\n",
    "Install the necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install promptflow-evals\n",
    "%pip install --upgrade promptflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built-in evaluators\n",
    "\n",
    "Built-in evaluators support the following application scenarios:\n",
    "- Question and answer\n",
    "- Chat\n",
    "\n",
    "> Note: It is recommended to use GPT models that do not have the (preview) suffix for the best performance and parseable responses with the evaluators.\n",
    "\n",
    "| Category | Evaluator class |\n",
    "|----------|-----------------|\n",
    "| Performance and quality | `GroundednessEvaluator`, `RelevanceEvaluator`, `CoherenceEvaluator`, `FluencyEvaluator`, `SimilarityEvaluator`, `F1ScoreEvaluator` |\n",
    "| Risk and safety | `ViolenceEvaluator`, `SexualEvaluator`, `SelfHarmEvaluator`, `HateUnfairnessEvaluator` |\n",
    "| Composite | `QAEvaluator`, `ChatEvaluator`, `ContentSafetyEvaluator`, `ContentSafetyChatEvaluator` |\n",
    "\n",
    "See the required inputs for built-in evaluators [here](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/flow-evaluate-sdk#required-data-input-for-built-in-evaluators)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance and Quality Evaluators\n",
    "This is an example of a relevence evaluator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://gpt4-0sweden.openai.azure.com/\n",
      "{\n",
      "    \"gpt_relevance\": 5.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from promptflow.core import AzureOpenAIModelConfiguration\n",
    "\n",
    "load_dotenv(\"credentials.env\")\n",
    "print(os.getenv(\"AZURE_OPENAI_ENDPOINT\"))\n",
    "\n",
    "# Initialize Azure OpenAI Connection\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")\n",
    "from promptflow.evals.evaluators import RelevanceEvaluator\n",
    "\n",
    "# Initialzing Relevance Evaluator\n",
    "relevance_eval = RelevanceEvaluator(model_config)\n",
    "# Running Relevance Evaluator on single input row\n",
    "relevance_score = relevance_eval(\n",
    "    answer=\"The Alpine Explorer Tent is the most waterproof.\",\n",
    "    context=\"From the our product list,\"\n",
    "    \" the alpine explorer tent is the most waterproof.\"\n",
    "    \" The Adventure Dining Table has higher weight.\",\n",
    "    question=\"Which tent is the most waterproof?\",\n",
    ")\n",
    "print(json.dumps(relevance_score, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risk and safety evaluators\n",
    "\n",
    "> Note: to use this you must be logged into Azure in your current IDE, eg VsCode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install azure-identity\n",
    "\n",
    "#You may need to log into your tenant through the Azure CLI to authenticate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import TokenCredential\n",
    "from azure.identity import DefaultAzureCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-06 13:53:54 +0100][flowinvoker][INFO] - Validating flow input with data {'metric_name': 'violence', 'question': 'What is the capital of France?', 'answer': 'Paris.', 'project_scope': {'subscription_id': '13e90e3a-50db-43f8-86e5-42bca5b14ebd', 'resource_group_name': 'rg-aistudio-35hub', 'project_name': 'sk-project'}, 'credential': None}\n",
      "[2024-08-06 13:53:54 +0100][flowinvoker][INFO] - Execute flow with data {'metric_name': 'violence', 'question': 'What is the capital of France?', 'answer': 'Paris.', 'project_scope': {'subscription_id': '13e90e3a-50db-43f8-86e5-42bca5b14ebd', 'resource_group_name': 'rg-aistudio-35hub', 'project_name': 'sk-project'}, 'credential': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-06 13:53:54 +0100   32504 execution.flow     INFO     Start executing nodes in thread pool mode.\n",
      "2024-08-06 13:53:54 +0100   32504 execution.flow     INFO     Start to run 2 nodes with concurrency level 16.\n",
      "2024-08-06 13:53:54 +0100   32504 execution.flow     INFO     Executing node validate_inputs. node run id: 2f7c3cfe-3562-4c01-a455-b76fb67fc062_validate_inputs_898e9d4a-c638-4043-a111-864d0b865a02\n",
      "2024-08-06 13:53:54 +0100   32504 execution.flow     INFO     Node validate_inputs completes.\n",
      "2024-08-06 13:53:54 +0100   32504 execution.flow     INFO     The node 'evaluate_with_rai_service' will be executed because the activate condition is met, i.e. '${validate_inputs.output}' is equal to 'True'.\n",
      "2024-08-06 13:53:54 +0100   32504 execution.flow     INFO     Executing node evaluate_with_rai_service. node run id: 2f7c3cfe-3562-4c01-a455-b76fb67fc062_evaluate_with_rai_service_0100095b-6727-49dc-bad5-03a8c855e7dc\n",
      "2024-08-06 13:54:04 +0100   32504 execution.flow     INFO     Node evaluate_with_rai_service completes.\n",
      "{\n",
      "    \"violence\": \"Very low\",\n",
      "    \"violence_score\": 0,\n",
      "    \"violence_reason\": \"The system's response provides factual information about the capital of France, which contains no violent content or references to violence.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#replace with your own project scope\n",
    "project_scope = {\n",
    "   \"subscription_id\": \"13e90e3a-50db-43f8-86e5-42bca5b14ebd\",\n",
    "    \"resource_group_name\": \"rg-aistudio-35hub\",\n",
    "    \"project_name\": \"sk-project\"\n",
    "}\n",
    "\n",
    "\n",
    "from promptflow.evals.evaluators import ViolenceEvaluator\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Initialize the credential variable\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# Initializing Violence Evaluator with project information\n",
    "violence_eval = ViolenceEvaluator(project_scope)\n",
    "\n",
    "# Running Violence Evaluator on single input row\n",
    "violence_score = violence_eval(question=\"What is the capital of France?\", answer=\"Paris.\")\n",
    "print(json.dumps(violence_score, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composite evaluators\n",
    "Composite evaluators are built in evaluators that combine the individual quality or safety metrics to easily provide a wide range of metrics right out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iancurtis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\promptflow\\evals\\evaluators\\_chat\\_chat.py:225: RuntimeWarning: Mean of empty slice\n",
      "  aggregated[metric] = np.nanmean(values)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"evaluation_per_turn\": {\n",
      "        \"gpt_relevance\": {\n",
      "            \"score\": [\n",
      "                5.0\n",
      "            ]\n",
      "        },\n",
      "        \"gpt_groundedness\": {\n",
      "            \"score\": [\n",
      "                5.0\n",
      "            ]\n",
      "        },\n",
      "        \"gpt_coherence\": {\n",
      "            \"score\": [\n",
      "                5.0\n",
      "            ]\n",
      "        },\n",
      "        \"gpt_fluency\": {\n",
      "            \"score\": [\n",
      "                NaN\n",
      "            ]\n",
      "        },\n",
      "        \"gpt_retrieval\": {\n",
      "            \"score\": [\n",
      "                5.0\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"gpt_coherence\": 5.0,\n",
      "    \"gpt_fluency\": NaN,\n",
      "    \"gpt_groundedness\": 5.0,\n",
      "    \"gpt_relevance\": 5.0,\n",
      "    \"gpt_retrieval\": 5.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from promptflow.evals.evaluators import ChatEvaluator\n",
    "\n",
    "chat_evaluator = ChatEvaluator(\n",
    "    model_config=model_config,\n",
    "    eval_last_turn=True\n",
    "  )\n",
    "\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the value of 2 + 2?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"2 + 2 = 4\", \"context\": {\n",
    "        \"citations\": [\n",
    "                {\"id\": \"math_doc.md\", \"content\": \"Information about additions: 1 + 2 = 3, 2 + 2 = 4\"}\n",
    "                ]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "result = chat_evaluator(conversation=conversation)\n",
    "\n",
    "print(json.dumps(result, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom evaluators\n",
    "Custom evaluators allow you to build your own code-based or prompt-based evaluator.\n",
    "\n",
    "### Code-based evaluators\n",
    "This is when code-based evaluators let you define metrics based on functions or a callable class. Given a simple Python class in an example `answer_length.py` that calculates the length of an answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"answer_length.py\", mode='r') as fin:\n",
    "    print(fin.read())\n",
    "from answer_length import AnswerLengthEvaluator\n",
    "\n",
    "answer_length = AnswerLengthEvaluator(answer=\"What is the speed of light?\")\n",
    "\n",
    "print(answer_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt-based evaluators\n",
    "You can create your own prompty-based evaluator and run it on a row of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "name: Apology Evaluator\n",
      "description: Apology Evaluator for QA scenario\n",
      "model:\n",
      "  api: chat\n",
      "  configuration:\n",
      "    type: azure_openai\n",
      "    connection: open_ai_connection\n",
      "    azure_deployment: gpt-4\n",
      "  parameters:\n",
      "    temperature: 0.2\n",
      "    response_format: { \"type\": \"text\" }\n",
      "inputs:\n",
      "  question:\n",
      "    type: string\n",
      "  answer:\n",
      "    type: string\n",
      "outputs:\n",
      "  apology:\n",
      "    type: int\n",
      "---\n",
      "system:\n",
      "You are an AI tool that determines if, in a chat conversation, the assistant apologized, like say sorry.\n",
      "Only provide a response of {\"apology\": 0} or {\"apology\": 1} so that the output is valid JSON.\n",
      "Give a apology of 1 if apologized in the chat conversation.\n",
      "{\"apology\": 0}\n"
     ]
    }
   ],
   "source": [
    "with open(\"apology.prompty\") as fin:\n",
    "    print(fin.read())\n",
    "from promptflow.client import load_flow\n",
    "\n",
    "# load apology evaluator from prompty file using promptflow\n",
    "apology_eval = load_flow(source=\"apology.prompty\", model={\"configuration\": model_config})\n",
    "apology_score = apology_eval(\n",
    "    question=\"What is the capital of France?\", answer=\"Paris\"\n",
    ")\n",
    "print(apology_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test dataset using `evaluate()`\n",
    "Use `evaluate()` to combine multiple evaluators on an entire test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install promptflow-azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install promptflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluate import evaluate\n",
    "\n",
    "result = evaluate(\n",
    "    data=\"./data/data.jsonl\", # provide your data here\n",
    "    evaluators={\n",
    "        \"relevance\": relevance_eval\n",
    "        },\n",
    "    # column mapping\n",
    "    evaluator_config={\n",
    "        \"default\": {\n",
    "            \"ground_truth\": \"${data.truth}\"\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[mslearn link](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/flow-evaluate-sdk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
