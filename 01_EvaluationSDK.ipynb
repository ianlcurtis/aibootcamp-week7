{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies\n",
    "\n",
    "First install the evaluators package from prompt flow SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install promptflow-evals\n",
    "%pip install --upgrade promptflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built-in evaluators\n",
    "\n",
    "Built-in evaluators support the following application scenarios:\n",
    "\n",
    "- Question and answer: This scenario is designed for applications that involve sending in queries and generating answers.\n",
    "- Chat: This scenario is suitable for applications where the model engages in conversation using a retrieval-augmented approach to extract information from your provided documents and generate detailed responses.\n",
    "\n",
    "\n",
    "| Category | Evaluator class |\n",
    "|----------|-----------------|\n",
    "| Performance and quality | `GroundednessEvaluator`, `RelevanceEvaluator`, `CoherenceEvaluator`, `FluencyEvaluator`, `SimilarityEvaluator`, `F1ScoreEvaluator` |\n",
    "| Risk and safety | `ViolenceEvaluator`, `SexualEvaluator`, `SelfHarmEvaluator`, `HateUnfairnessEvaluator` |\n",
    "| Composite\t| `QAEvaluator`, `ChatEvaluator`, `ContentSafetyEvaluator`, `ContentSafetyChatEvaluator` |\n",
    "\n",
    "Both categories of built-in quality and safety metrics take in question and answer pairs, along with additional information for specific evaluators.\n",
    "\n",
    "Built-in composite evaluators are composed of individual evaluators.\n",
    "\n",
    "- `QAEvaluator` combines all the quality evaluators for a single output of combined metrics for question and answer pairs\n",
    "- `ChatEvaluator` combines all the quality evaluators for a single output of combined metrics for chat messages following the OpenAI message protocol that can be found here. In addition to all the quality evaluators, we include support for retrieval score. Retrieval score isn't currently supported as a standalone evaluator class.\n",
    "- `ContentSafetyEvaluator` combines all the safety evaluators for a single output of combined metrics for question and answer pairs\n",
    "- `ContentSafetyChatEvaluator` combines all the safety evaluators for a single output of combined metrics for chat messages following the OpenAI message protocol that can be found here.\n",
    "\n",
    "\n",
    "# Required data input for built-in evaluators\n",
    "We require question and answer pairs in .jsonl format with the required inputs, and column mapping for evaluating datasets, as follows:\n",
    "\n",
    "| Evaluator | question | answer | context | ground_truth |\n",
    "|-----------|----------|--------|---------|------------- |\n",
    "| `GroundednessEvaluator`\t| N/A |\tRequired: String | Required: String | N/A |\n",
    "| `RelevanceEvaluator` | Required: String | Required: String | Required: String | N/A |\n",
    "| `CoherenceEvaluator`\t| Required: String | Required: String | N/A | N/A |\n",
    "| `FluencyEvaluator`\t| Required: String | Required: String | N/A\tN/A |\n",
    "| `SimilarityEvaluator` | Required: String | Required: String | N/A |Required: String |\n",
    "| `F1ScoreEvaluator` | N/A | Required: String | N/A | Required: String |\n",
    "| `ViolenceEvaluator` | Required: String | Required: String | N/A | N/A |\n",
    "| `SexualEvaluator` | Required: String | Required: String | N/A | N/A |\n",
    "| `SelfHarmEvaluator` | Required: String | Required: String | N/A | N/A |\n",
    "| `HateUnfairnessEvaluator` | Required: String | Required: String | N/A | N/A |\n",
    "\n",
    "- Question: the question sent in to the generative AI application\n",
    "- Answer: the response to question generated by the generative AI application\n",
    "- Context: the source that response is generated with respect to (that is, grounding documents)\n",
    "- Ground truth: the response to question generated by user/human as the true answer\n",
    "\n",
    "# Performance and quality evaluators\n",
    "When using AI-assisted performance and quality metrics, you must specify a GPT model for the calculation process. Choose a deployment with either GPT-3.5, GPT-4, or the Davinci model for your calculations and set it as your `model_config`.\n",
    "\n",
    "> Note: \n",
    "We recommend using GPT models that do not have the (preview) suffix for the best performance and parseable responses with our evaluators.\n",
    "\n",
    "You can run the built-in evaluators by importing the desired evaluator class. Ensure that you set your environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from promptflow.core import AzureOpenAIModelConfiguration\n",
    "\n",
    "load_dotenv(\"credentials.env\")\n",
    "print(os.getenv(\"AZURE_OPENAI_ENDPOINT\"))\n",
    "\n",
    "# Initialize Azure OpenAI Connection with your environment variables\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")\n",
    "from promptflow.evals.evaluators import RelevanceEvaluator\n",
    "\n",
    "# Initialzing Relevance Evaluator\n",
    "relevance_eval = RelevanceEvaluator(model_config)\n",
    "# Running Relevance Evaluator on single input row\n",
    "relevance_score = relevance_eval(\n",
    "    answer=\"The Alpine Explorer Tent is the most waterproof.\",\n",
    "    context=\"From the our product list,\"\n",
    "    \" the alpine explorer tent is the most waterproof.\"\n",
    "    \" The Adventure Dining Table has higher weight.\",\n",
    "    question=\"Which tent is the most waterproof?\",\n",
    ")\n",
    "print(relevance_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk and safety evaluators\n",
    "When you use AI-assisted risk and safety metrics, a GPT model isn't required. Instead of `model_config`, provide your `azure_ai_project` information. This accesses the Azure AI Studio safety evaluations back-end service, which provisions a GPT-4 model that can generate content risk severity scores and reasoning to enable your safety evaluators. Note that to use this you must be logged into Azure in your current IDE, eg VsCode. \n",
    "\n",
    "> Note: \n",
    "Currently AI-assisted risk and safety metrics are only available in the following regions: East US 2, France Central, UK South, Sweden Central. Groundedness measurement leveraging Azure AI Content Safety Groundedness Detection is only supported following regions: East US 2 and Sweden Central. Read more about the supported metrics here and when to use which metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install azure-identity\n",
    "\n",
    "#You may need to log into your tenant through the Azure CLI to authenticate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import TokenCredential\n",
    "from azure.identity import DefaultAzureCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_scope = {\n",
    "   \"subscription_id\": \"13e90e3a-50db-43f8-86e5-42bca5b14ebd\",\n",
    "    \"resource_group_name\": \"rg-aistudio-35hub\",\n",
    "    \"project_name\": \"sk-project\"\n",
    "}\n",
    "\n",
    "\n",
    "from promptflow.evals.evaluators import ViolenceEvaluator\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Initialize the credential variable\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# Initializing Violence Evaluator with project information\n",
    "violence_eval = ViolenceEvaluator(project_scope)\n",
    "\n",
    "# Running Violence Evaluator on single input row\n",
    "violence_score = violence_eval(question=\"What is the capital of France?\", answer=\"Paris.\")\n",
    "print(violence_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the safety evaluators is a dictionary containing:\n",
    "\n",
    "- `{metric_name}` provides a severity label for that content risk ranging from Very low, Low, Medium, and High.\n",
    "- `{metric_name}_score` has a range between 0 and 7 severity level that maps to a severity label given in {metric_name}.\n",
    "- `{metric_name}_reason` has a text reasoning for why a certain severity score was given for each data point.\n",
    "\n",
    "## Evaluating jailbreak vulnerability\n",
    "Evaluating jailbreak is a comparative measurement, not an AI-assisted metric. Run `ContentSafetyEvaluator` or `ContentSafetyChatEvaluator` on two different, red-teamed datasets: a baseline adversarial test dataset versus the same adversarial test dataset with jailbreak injections in the first turn. You can do this with functionality and attack datasets generated with the adversarial simulator. Then you can evaluate jailbreak vulnerability by comparing results from content safety evaluators between the two test dataset's aggregate scores for each safety evaluator.\n",
    "\n",
    "# Composite evaluators\n",
    "Composite evaluators are built in evaluators that combine the individual quality or safety metrics to easily provide a wide range of metrics right out of the box.\n",
    "\n",
    "The `ChatEvaluator` class provides quality metrics for evaluating chat messages, therefore there's an optional flag to indicate that you only want to evaluate on the last turn of a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluators import ChatEvaluator\n",
    "\n",
    "chat_evaluator = ChatEvaluator(\n",
    "    model_config=model_config,\n",
    "    eval_last_turn=True\n",
    "  )\n",
    "\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the value of 2 + 2?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"2 + 2 = 4\", \"context\": {\n",
    "        \"citations\": [\n",
    "                {\"id\": \"math_doc.md\", \"content\": \"Information about additions: 1 + 2 = 3, 2 + 2 = 4\"}\n",
    "                ]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "result = chat_evaluator(conversation=conversation)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom evaluators\n",
    "Built-in evaluators are great out of the box to start evaluating your application's generations. However you might want to build your own code-based or prompt-based evaluator to cater to your specific evaluation needs.\n",
    "\n",
    "## Code-based evaluators\n",
    "Sometimes a large language model isn't needed for certain evaluation metrics. This is when code-based evaluators can give you the flexibility to define metrics based on functions or callable class. Given a simple Python class in an example `answer_length.py` that calculates the length of an answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"answer_length.py\", mode='r') as fin:\n",
    "    print(fin.read())\n",
    "from answer_length import AnswerLengthEvaluator\n",
    "\n",
    "answer_length = AnswerLengthEvaluator(answer=\"What is the speed of light?\")\n",
    "\n",
    "print(answer_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt-based evaluators\n",
    "To build your own prompt-based large language model evaluator, you can create a custom evaluator based on a Prompty file. Prompty is a file with `.prompty` extension for developing prompt template. The Prompty asset is a markdown file with a modified front matter. The front matter is in YAML format that contains many metadata fields that define model configuration and expected inputs of the Prompty. Use the example `apology.prompty` file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create your own prompty-based evaluator and run it on a row of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"apology.prompty\") as fin:\n",
    "    print(fin.read())\n",
    "from promptflow.client import load_flow\n",
    "\n",
    "# load apology evaluator from prompty file using promptflow\n",
    "apology_eval = load_flow(source=\"apology.prompty\", model={\"configuration\": model_config})\n",
    "apology_score = apology_eval(\n",
    "    question=\"What is the capital of France?\", answer=\"Paris\"\n",
    ")\n",
    "print(apology_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test dataset using `evaluate()`\n",
    "After you spot-check your built-in or custom evaluators on a single row of data, you can combine multiple evaluators with the `evaluate()` API on an entire test dataset. In order to ensure the `evaluate()` can correctly parse the data, you must specify column mapping to map the column from the dataset to key words that are accepted by the evaluators. In this case, we specify the data mapping for `ground_truth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install promptflow-azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install promptflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluate import evaluate\n",
    "\n",
    "result = evaluate(\n",
    "    data=\"./data/data.jsonl\", # provide your data here\n",
    "    evaluators={\n",
    "        \"relevance\": relevance_eval\n",
    "        },\n",
    "    # column mapping\n",
    "    evaluator_config={\n",
    "        \"default\": {\n",
    "            \"ground_truth\": \"${data.truth}\"\n",
    "        }\n",
    "    }\n",
    "    # Optionally provide your AI Studio project information to track your evaluation results in your Azure AI studio project\n",
    "    #azure_ai_project = azure_ai_project,\n",
    "    # Optionally provide an output path to dump a json of metric summary, row level data and metric and studio URL\n",
    "    #output_path=\"./myevalresults.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tip: Get the contents of the `result.studio_url` property for a link to view your logged evaluation results in Azure AI Studio. The evaluator outputs results in a dictionary which contains aggregate `metrics` and row-level data and metrics. An example of an output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'metrics': {'answer_length.value': 49.333333333333336,\n",
    "             'relevance.gpt_relevance': 5.0},\n",
    " 'rows': [{'inputs.answer': 'Paris is the capital of France.',\n",
    "           'inputs.context': 'France is in Europe',\n",
    "           'inputs.ground_truth': 'Paris has been the capital of France since '\n",
    "                                  'the 10th century and is known for its '\n",
    "                                  'cultural and historical landmarks.',\n",
    "           'inputs.question': 'What is the capital of France?',\n",
    "           'outputs.answer_length.value': 31,\n",
    "           'outputs.relevance.gpt_relevance': 5},\n",
    "          {'inputs.answer': 'Albert Einstein developed the theory of '\n",
    "                            'relativity.',\n",
    "           'inputs.context': 'The theory of relativity is a foundational '\n",
    "                             'concept in modern physics.',\n",
    "           'inputs.ground_truth': 'Albert Einstein developed the theory of '\n",
    "                                  'relativity, with his special relativity '\n",
    "                                  'published in 1905 and general relativity in '\n",
    "                                  '1915.',\n",
    "           'inputs.question': 'Who developed the theory of relativity?',\n",
    "           'outputs.answer_length.value': 51,\n",
    "           'outputs.relevance.gpt_relevance': 5},\n",
    "          {'inputs.answer': 'The speed of light is approximately 299,792,458 '\n",
    "                            'meters per second.',\n",
    "           'inputs.context': 'Light travels at a constant speed in a vacuum.',\n",
    "           'inputs.ground_truth': 'The exact speed of light in a vacuum is '\n",
    "                                  '299,792,458 meters per second, a constant '\n",
    "                                  \"used in physics to represent 'c'.\",\n",
    "           'inputs.question': 'What is the speed of light?',\n",
    "           'outputs.answer_length.value': 66,\n",
    "           'outputs.relevance.gpt_relevance': 5}],\n",
    " 'traces': {}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supported data formats for `evaluate()`\n",
    "The `evaluate()` API only accepts data in the JSONLines format. For all built-in evaluators, except for `ChatEvaluator` or `ContentSafetyChatEvaluator`, `evaluate()` requires data in the following format with required input fields. See the previous section on required data input for built-in evaluators.\n",
    "\n",
    "``` json\n",
    "{\n",
    "  \"question\":\"What is the capital of France?\",\n",
    "  \"context\":\"France is in Europe\",\n",
    "  \"answer\":\"Paris is the capital of France.\",\n",
    "  \"ground_truth\": \"Paris\"\n",
    "}\n",
    "```\n",
    "\n",
    "For the composite evaluator class, `ChatEvaluator` and `ContentSafetyChatEvaluator`, we require an array of messages that adheres to OpenAI's messages protocol that can be found here. The messages protocol contains a role-based list of messages with the following:\n",
    "\n",
    "- `content`: The content of that turn of the interaction between user and application or assistant.\n",
    "- `role`: Either the user or application/assistant.\n",
    "- `\"citations\"` (within `\"context\"`): Provides the documents and its ID as key value pairs from the retrieval-augmented generation model.\n",
    "\n",
    "| Evaluator class |\tCitations from retrieved documents |\n",
    "|-----------------|------------------------------------|\n",
    "| `GroundednessEvaluator` |\tRequired: String |\n",
    "| `RelevanceEvaluator` | Required: String |\n",
    "| `CoherenceEvaluator` | N/A |\n",
    "| `FluencyEvaluator` | N/A |\n",
    "\n",
    "Citations: the relevant source from retrieved documents by retrieval model or user provided context that model's answer is generated with respect to.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"content\": \"<conversation_turn_content>\", \n",
    "            \"role\": \"<role_name>\", \n",
    "            \"context\": {\n",
    "                \"citations\": [\n",
    "                    {\n",
    "                        \"id\": \"<content_key>\",\n",
    "                        \"content\": \"<content_value>\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "To `evaluate()` with either the `ChatEvaluator` or `ContentSafetyChatEvaluator`, ensure in the data mapping you match the key `messages` to your array of messages, given that your data adheres to the chat protocol defined above:\n",
    "\n",
    "``` python\n",
    "result = evaluate(\n",
    "    data=\"data.jsonl\",\n",
    "    evaluators={\n",
    "        \"chatevaluator\": chat_evaluator\n",
    "    },\n",
    "    # column mapping for messages\n",
    "    evaluator_config={\n",
    "        \"default\": {\n",
    "            \"messages\": \"${data.messages}\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on a target\n",
    "If you have a list of queries that you'd like to run then evaluate, the `evaluate()` also supports a `target parameter`, which can send queries to an application to collect answers then run your evaluators on the resulting question and answers.\n",
    "\n",
    "A target can be any callable class in your directory. In this case we have a python script `askwiki.py` with a callable class `askwiki()` that we can set as our target. Given a dataset of queries we can send into our simple `askwiki` app, we can evaluate the relevance of the outputs.\n",
    "\n",
    "``` python\n",
    "from askwiki import askwiki\n",
    "\n",
    "result = evaluate(\n",
    "    data=\"data.jsonl\",\n",
    "    target=askwiki,\n",
    "    evaluators={\n",
    "        \"relevance\": relevance_eval\n",
    "    },\n",
    "    evaluator_config={\n",
    "        \"default\": {\n",
    "            \"question\": \"${data.queries}\"\n",
    "            \"context\": \"${outputs.context}\"\n",
    "            \"answer\": \"${outputs.response}\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
