{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Required Libraries\n",
    "Install the necessary Python libraries and load settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-contentsafety\n",
    "%pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"credentials.env\")\n",
    "api_key=os.getenv('AZURE_CONTENT_SAFETY_KEY')\n",
    "endpoint=os.getenv('AZURE_CONTENT_SAFETY_ENDPOINT')\n",
    "\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Image Moderation Function\n",
    "Scan images and detect potential adult and racy content by using tags, confidence scores, and other extracted information. Code to define a function that takes an image URL as input and calls the Azure Content Safety API for moderation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from azure.ai.contentsafety import ContentSafetyClient\n",
    "from azure.ai.contentsafety.models import AnalyzeImageOptions, ImageData, ImageCategory\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "\n",
    "def analyze_image():\n",
    "    endpoint = os.environ.get('AZURE_CONTENT_SAFETY_ENDPOINT')\n",
    "    key = os.environ.get('AZURE_CONTENT_SAFETY_KEY')\n",
    "    image_path = os.path.join(\"sample\", \"knife.jpg\")\n",
    "\n",
    "    # Create an Azure AI Content Safety client\n",
    "    client = ContentSafetyClient(endpoint, AzureKeyCredential(key))\n",
    "\n",
    "\n",
    "    # Build request\n",
    "    with open(image_path, \"rb\") as file:\n",
    "        request = AnalyzeImageOptions(image=ImageData(content=file.read()))\n",
    "\n",
    "    # Analyze image\n",
    "    try:\n",
    "        response = client.analyze_image(request)\n",
    "    except HttpResponseError as e:\n",
    "        print(\"Analyze image failed.\")\n",
    "        if e.error:\n",
    "            print(f\"Error code: {e.error.code}\")\n",
    "            print(f\"Error message: {e.error.message}\")\n",
    "            raise\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    hate_result = next(item for item in response.categories_analysis if item.category == ImageCategory.HATE)\n",
    "    self_harm_result = next(item for item in response.categories_analysis if item.category == ImageCategory.SELF_HARM)\n",
    "    sexual_result = next(item for item in response.categories_analysis if item.category == ImageCategory.SEXUAL)\n",
    "    violence_result = next(item for item in response.categories_analysis if item.category == ImageCategory.VIOLENCE)\n",
    "\n",
    "    if hate_result:\n",
    "        print(f\"Hate severity: {hate_result.severity}\")\n",
    "    if self_harm_result:\n",
    "        print(f\"SelfHarm severity: {self_harm_result.severity}\")\n",
    "    if sexual_result:\n",
    "        print(f\"Sexual severity: {sexual_result.severity}\")\n",
    "    if violence_result:\n",
    "        print(f\"Violence severity: {violence_result.severity}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Text Moderation Function\n",
    "Scan text content. Profanity terms and personal data are returned. Code to define a function that takes text as input and calls the Azure Content Safety API for moderation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License. See License.txt in the project root for\n",
    "# license information.\n",
    "# --------------------------------------------------------------------------\n",
    "import os\n",
    "from azure.ai.contentsafety import ContentSafetyClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "from azure.ai.contentsafety.models import AnalyzeTextOptions, TextCategory\n",
    "\n",
    "\n",
    "# Sample: Analyze text in sync request\n",
    "def analyze_text():\n",
    "    # analyze text\n",
    "    key = os.environ[\"AZURE_CONTENT_SAFETY_KEY\"]\n",
    "    endpoint = os.environ[\"AZURE_CONTENT_SAFETY_ENDPOINT\"]\n",
    "\n",
    "    # Create a Content Safety client\n",
    "    client = ContentSafetyClient(endpoint, AzureKeyCredential(key))\n",
    "\n",
    "    # Construct request\n",
    "    request = AnalyzeTextOptions(text=\"You are an idiot\")\n",
    "\n",
    "    # Analyze text\n",
    "    try:\n",
    "        response = client.analyze_text(request)\n",
    "    except HttpResponseError as e:\n",
    "        print(\"Analyze text failed.\")\n",
    "        if e.error:\n",
    "            print(f\"Error code: {e.error.code}\")\n",
    "            print(f\"Error message: {e.error.message}\")\n",
    "            raise\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    hate_result = next(item for item in response.categories_analysis if item.category == TextCategory.HATE)\n",
    "    self_harm_result = next(item for item in response.categories_analysis if item.category == TextCategory.SELF_HARM)\n",
    "    sexual_result = next(item for item in response.categories_analysis if item.category == TextCategory.SEXUAL)\n",
    "    violence_result = next(item for item in response.categories_analysis if item.category == TextCategory.VIOLENCE)\n",
    "\n",
    "    if hate_result:\n",
    "        print(f\"Hate severity: {hate_result.severity}\")\n",
    "    if self_harm_result:\n",
    "        print(f\"SelfHarm severity: {self_harm_result.severity}\")\n",
    "    if sexual_result:\n",
    "        print(f\"Sexual severity: {sexual_result.severity}\")\n",
    "    if violence_result:\n",
    "        print(f\"Violence severity: {violence_result.severity}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install video dependencies\n",
    "%pip install decord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Video Moderation Function\n",
    "Scan videos and detect potential adult and racy content. Code to define a function that takes video as input and calls the Azure Content Safety API for moderation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License. See License.txt in the project root for\n",
    "# license information.\n",
    "# --------------------------------------------------------------------------\n",
    "import os\n",
    "\n",
    "from azure.ai.contentsafety import ContentSafetyClient\n",
    "from azure.ai.contentsafety.models import AnalyzeImageOptions, ImageData, ImageCategory\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "\n",
    "import datetime\n",
    "from decord import VideoReader\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Sample: Analyze video in sync request\n",
    "def analyze_video():\n",
    "    # [START analyze_video]\n",
    "    key = os.environ[\"AZURE_CONTENT_SAFETY_KEY\"]\n",
    "    endpoint = os.environ[\"AZURE_CONTENT_SAFETY_ENDPOINT\"]\n",
    "    video_path = os.path.join(\"sample\", \"video.mp4\")\n",
    "    # Create a Content Safety client\n",
    "    client = ContentSafetyClient(endpoint, AzureKeyCredential(key))\n",
    "\n",
    "    # Read the video\n",
    "    video = VideoReader(video_path)\n",
    "    sampling_fps = 1  # process at 1 frames per second\n",
    "    key_frames = video.get_batch(\n",
    "        np.int_(np.arange(0, len(video), video.get_avg_fps() / sampling_fps))).asnumpy()\n",
    "\n",
    "    # Iterate over key frames\n",
    "    for key_frame_idx in tqdm(range(len(key_frames)), desc=\"Processing video\",\n",
    "                              total=len(key_frames)):\n",
    "        frame = Image.fromarray(key_frames[key_frame_idx])\n",
    "        frame_bytes = BytesIO()\n",
    "        frame.save(frame_bytes, format=\"PNG\")\n",
    "\n",
    "        # Build request\n",
    "        request = AnalyzeImageOptions(image=ImageData(content=frame_bytes.getvalue()))\n",
    "\n",
    "        # Analyze image\n",
    "        frame_time_ms = key_frame_idx * 1000 / sampling_fps\n",
    "        frame_timestamp = datetime.timedelta(milliseconds=frame_time_ms)\n",
    "        print(f\"Analyzing video at {frame_timestamp}\")\n",
    "        try:\n",
    "            response = client.analyze_image(request)\n",
    "        except HttpResponseError as e:\n",
    "            print(f\"Analyze video failed at {frame_timestamp}\")\n",
    "            if e.error:\n",
    "                print(f\"Error code: {e.error.code}\")\n",
    "                print(f\"Error message: {e.error.message}\")\n",
    "                raise\n",
    "            print(e)\n",
    "            raise\n",
    "\n",
    "        hate_result = next(\n",
    "            item for item in response.categories_analysis if item.category == ImageCategory.HATE)\n",
    "        self_harm_result = next(item for item in response.categories_analysis if\n",
    "                                item.category == ImageCategory.SELF_HARM)\n",
    "        sexual_result = next(\n",
    "            item for item in response.categories_analysis if item.category == ImageCategory.SEXUAL)\n",
    "        violence_result = next(item for item in response.categories_analysis if\n",
    "                               item.category == ImageCategory.VIOLENCE)\n",
    "\n",
    "        if hate_result:\n",
    "            print(f\"Hate severity: {hate_result.severity}\")\n",
    "        if self_harm_result:\n",
    "            print(f\"SelfHarm severity: {self_harm_result.severity}\")\n",
    "        if sexual_result:\n",
    "            print(f\"Sexual severity: {sexual_result.severity}\")\n",
    "        if violence_result:\n",
    "            print(f\"Violence severity: {violence_result.severity}\")\n",
    "\n",
    "    # [END analyze_video]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[mslearn link](https://learn.microsoft.com/en-us/azure/ai-services/content-moderator/api-reference)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
